name: drive

services:
  postgresql:
    image: postgres:16
    ports:
      - "6434:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -d $${POSTGRES_DB} -U $${POSTGRES_USER}"]
      interval: 1s
      timeout: 2s
      retries: 300
    env_file:
      - env.d/development/postgresql
      - env.d/development/postgresql.${ENV_OVERRIDE:-local}
    volumes:
      - ./data/postgresql.${ENV_OVERRIDE:-local}:/var/lib/postgresql/data

  redis:
    image: redis:5
    ports:
      - "6379:6379"

  mailcatcher:
    image: sj26/mailcatcher:latest
    ports:
      - "1081:1080"

  # Baseline S3 provider (Sprint 0): SeaweedFS S3 gateway
  seaweedfs-master:
    image: chrislusf/seaweedfs:latest
    command: ["master", "-ip=seaweedfs-master", "-port=9333", "-mdir=/data"]
    volumes:
      - ./data/seaweedfs/master:/data

  seaweedfs-volume:
    image: chrislusf/seaweedfs:latest
    command:
      [
        "volume",
        "-ip=seaweedfs-volume",
        "-port=8080",
        "-dir=/data",
        "-mserver=seaweedfs-master:9333",
      ]
    depends_on:
      - seaweedfs-master
    volumes:
      - ./data/seaweedfs/volume:/data

  seaweedfs-filer:
    image: chrislusf/seaweedfs:latest
    command: ["filer", "-ip=seaweedfs-filer", "-port=8888", "-master=seaweedfs-master:9333"]
    depends_on:
      - seaweedfs-master
      - seaweedfs-volume
    volumes:
      - ./data/seaweedfs/filer:/data

  seaweedfs-s3:
    image: chrislusf/seaweedfs:latest
    command: ["s3", "-ip.bind=0.0.0.0", "-port=8333", "-filer=seaweedfs-filer:8888"]
    env_file:
      - env.d/development/seaweedfs-s3
    depends_on:
      - seaweedfs-filer
    ports:
      - "9000:8333"
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "code=$$(curl -s -o /dev/null -w '%{http_code}' http://localhost:8333/); test \"$$code\" = \"200\" -o \"$$code\" = \"403\"",
        ]
      interval: 1s
      timeout: 5s
      retries: 300

  createbuckets:
    image: minio/mc
    env_file:
      - env.d/development/common
    depends_on:
      seaweedfs-s3:
        condition: service_healthy
        restart: true
    entrypoint: >
      sh -c "
      /usr/bin/mc alias set drive http://seaweedfs-s3:8333 $${AWS_S3_ACCESS_KEY_ID} $${AWS_S3_SECRET_ACCESS_KEY} && \
      /usr/bin/mc mb -p drive/drive-media-storage || true && \
      /usr/bin/mc version enable drive/drive-media-storage || true && \
      exit 0;"

  # Optional fixture: MinIO (non-baseline; not used for blocking gates)
  minio:
    profiles: ["minio-fixture"]
    user: ${DOCKER_USER:-1000}
    image: minio/minio
    environment:
      - MINIO_ROOT_USER=drive
      - MINIO_ROOT_PASSWORD=password
    ports:
      # Published on 9100/9101 to avoid clashing with the baseline SeaweedFS gateway (9000).
      - "9100:9000"
      - "9101:9001"
    healthcheck:
      test: ["CMD", "mc", "ready", "local"]
      interval: 1s
      timeout: 20s
      retries: 300
    entrypoint: ""
    command: minio server --console-address :9001 /data
    volumes:
      - ./data/minio:/data

  createbuckets-minio:
    profiles: ["minio-fixture"]
    image: minio/mc
    env_file:
      - env.d/development/common
    depends_on:
      minio:
        condition: service_healthy
        restart: true
    entrypoint: >
      sh -c "
      /usr/bin/mc alias set drive http://minio:9000 $${AWS_S3_ACCESS_KEY_ID} $${AWS_S3_SECRET_ACCESS_KEY} && \
      /usr/bin/mc mb -p drive/drive-media-storage || true && \
      /usr/bin/mc version enable drive/drive-media-storage || true && \
      exit 0;"

  app-dev:
    build:
      context: .
      target: backend-development
      args:
        DOCKER_USER: ${DOCKER_USER:-1000}
    user: ${DOCKER_USER:-1000}
    image: drive:backend-development
    environment:
      - PYLINTHOME=/app/.pylint.d
      - DJANGO_CONFIGURATION=Development
    env_file:
      - env.d/development/common
      - env.d/development/common.${ENV_OVERRIDE:-local}
      - env.d/development/postgresql
      - env.d/development/postgresql.${ENV_OVERRIDE:-local}
    ports:
      - "8071:8000"
    volumes:
      - ./src/backend:/app
      - ./_bmad-output:/_bmad-output
      - ./data/static:/data/static
      - /app/.venv
    networks:
      - lasuite
      - default      
    depends_on:
      postgresql:
        condition: service_healthy
        restart: true
      mailcatcher:
        condition: service_started
      redis:
        condition: service_started
      createbuckets:
        condition: service_started
      celery-dev:
        condition: service_started
      collabora:
        condition: service_healthy
      onlyoffice:
        condition: service_healthy

  ds-proxy:
    image: demarchenumerique/ds-proxy:v2.0.0-alpha.2
    profiles: ["ds-proxy"]
    command: ["proxy", "--address", "0.0.0.0:4444", "--password-file", "/etc/dsproxy/PASSWORD", "--salt", "12345678901234567890123456789012", "--keyring-file", "/etc/dsproxy/keyring.toml", "--upstream-url", "http://seaweedfs-s3:8333", "--local-encryption-directory", "/var/tmp/local_encryption/", "--s3-access-key", "drive", "--s3-secret-key", "password", "--s3-region", "us-east-1"]
    ports:
      - "4444:4444"
    environment:
      - RUST_LOG=debug,ds_proxy::http::handlers::fetch=trace,ds_proxy::http::handlers::forward=trace
      - RUST_BACKTRACE=full
    volumes:
      - ./docker/files/development/ds_proxy/:/etc/dsproxy/:ro

  celery-dev:
    user: ${DOCKER_USER:-1000}
    image: drive:backend-development
    networks:
      - default
      - lasuite
    command: ["celery", "-A", "drive.celery_app", "worker", "-l", "DEBUG"]
    environment:
      - DJANGO_CONFIGURATION=Development
    env_file:
      - env.d/development/common
      - env.d/development/common.local
      - env.d/development/postgresql
      - env.d/development/postgresql.local
    volumes:
      - ./src/backend:/app
      - ./data/static:/data/static
      - /app/.venv

  nginx:
    image: nginx:1.25
    ports:
      - "8083:8083"
    networks:
      default: {}
      lasuite:
        aliases:
          - nginx
    volumes:
      - ./docker/files/development/etc/nginx/conf.d:/etc/nginx/conf.d:ro
    depends_on:
      - keycloak
      - app-dev

  frontend-dev:
    profiles:
      - frontend
    user: "${DOCKER_USER:-1000}"
    build:
      context: .
      dockerfile: ./src/frontend/Dockerfile
      target: drive-dev
      args:
        API_ORIGIN: "http://localhost:8071"
    image: drive:frontend-development
    volumes:
      - ./src/frontend/:/home/frontend/
      - /home/frontend/node_modules
      - /home/frontend/apps/drive/node_modules
    ports:
      - "3000:3000"

  crowdin:
    image: crowdin/cli:3.16.0
    volumes:
      - ".:/app"
    env_file:
      - env.d/development/crowdin
      - env.d/development/crowdin.local
    user: "${DOCKER_USER:-1000}"
    working_dir: /app

  node:
    image: node:22
    user: "${DOCKER_USER:-1000}"
    environment:
      HOME: /tmp
    volumes:
      - ".:/app"

  kc_postgresql:
    image: postgres:14.3
    healthcheck:
      test: ["CMD-SHELL", "pg_isready"]
      interval: 1s
      timeout: 2s
      retries: 300
    ports:
      - "6433:5432"
    env_file:
      - env.d/development/kc_postgresql
      - env.d/development/kc_postgresql.local

  keycloak:
    image: quay.io/keycloak/keycloak:26.3.2
    volumes:
      - ./docker/auth/realm.json:/opt/keycloak/data/import/realm.json
    command:
      - start-dev
      - --features=preview
      - --import-realm
      - --proxy=edge
      - --hostname=http://localhost:8083
      - --hostname-strict=false
    environment:
      KEYCLOAK_ADMIN: admin
      KEYCLOAK_ADMIN_PASSWORD: admin
      KC_DB: postgres
      KC_DB_URL_HOST: kc_postgresql
      KC_DB_URL_DATABASE: keycloak
      KC_DB_PASSWORD: pass
      KC_DB_USERNAME: drive
      KC_DB_SCHEMA: public
      PROXY_ADDRESS_FORWARDING: "true"
    ports:
      - "8080:8080"
    depends_on:
      - kc_postgresql
    networks:
      - lasuite
      - default

  collabora:
    image: collabora/code:latest
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9980/hosting/discovery"]
      interval: 30s
      retries: 5
      start_period: 60s
      timeout: 10s
    ports:
      - "9980:9980"
    environment:
      - extra_params=--o:ssl.enable=false
      - username=drive
      - password=password
      - server_name=localhost:9980
      - aliasgroup1=app-dev:8000

  onlyoffice:
    image: onlyoffice/documentserver-de
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost/hosting/discovery"]
      interval: 30s
      retries: 5
      start_period: 60s
      timeout: 10s
    environment:
      TZ: "Europe/Berlin"
      USE_UNAUTHORIZED_STORAGE: "true"
    ports:
      - "9981:80"
    volumes:
      - ./docker/onlyoffice/local-development.json:/etc/onlyoffice/documentserver/local-production-linux.json
      # Enable debug
      - ./docker/onlyoffice/log4js/production.json:/etc/onlyoffice/documentserver/log4js/production.json

networks:
  default: {}
  lasuite:
    name: lasuite-network
    driver: bridge
    external: true
